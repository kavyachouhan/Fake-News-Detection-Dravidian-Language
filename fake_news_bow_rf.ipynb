{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the data\n",
        "data = pd.read_csv('labelled_train_set.csv')\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# Drop any rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Convert text to lowercase\n",
        "data['News/Comment'] = data['News/Comment'].str.lower()\n",
        "\n",
        "# Encode the target variable\n",
        "data['Type'] = data['Type'].map({'FALSE': 0, 'HALF TRUE': 1, 'TRUE': 2})\n",
        "\n",
        "# Step 3: Split the data into features and target\n",
        "X = data['News/Comment']\n",
        "y = data['Type']\n",
        "\n",
        "# Handle Missing Values in Target Variable\n",
        "y = y.fillna(y.mode()[0]) # Fill missing values with the most frequent class\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 5: Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Handle class imbalance using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
        "\n",
        "# Step 7: Train the model\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Optional: Save the model and vectorizer for future use\n",
        "joblib.dump(rf_model, 'fake_news_model.pkl')\n",
        "joblib.dump(vectorizer, 'count_vectorizer.pkl')\n",
        "\n",
        "# ---------------------------------------\n",
        "# For making predictions on new test data\n",
        "# ---------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the saved model and vectorizer\n",
        "model = joblib.load('fake_news_model.pkl')\n",
        "vectorizer = joblib.load('count_vectorizer.pkl')\n",
        "\n",
        "# Step 2: Load the new test data\n",
        "test_data = pd.read_csv('unlabelled_test1.csv')  # Replace with your test data file path\n",
        "\n",
        "# Ensure that the test data contains an 'ID' column\n",
        "if 'ID' not in test_data.columns:\n",
        "    raise ValueError(\"Test data must contain an 'ID' column.\")\n",
        "\n",
        "# Step 3: Preprocess the test data\n",
        "# Convert text to lowercase\n",
        "test_data['News/Comment'] = test_data['News/Comment'].str.lower()\n",
        "\n",
        "# Step 4: Vectorize the test data\n",
        "X_test_vectorized = vectorizer.transform(test_data['News/Comment'])\n",
        "\n",
        "# Step 5: Make predictions\n",
        "predictions = model.predict(X_test_vectorized)\n",
        "\n",
        "# Step 6: Add predictions to the test data\n",
        "test_data['Predicted_Type'] = predictions\n",
        "\n",
        "# Step 7: Save the results with 'ID' and 'Predicted_Type'\n",
        "test_data[['ID', 'Predicted_Type']].to_csv('predicted_test_results.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'predicted_test_results.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oteHZDLaC4y-",
        "outputId": "7375a13f-96b2-46c0-d7c5-9be2e45a46db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.81      0.85       279\n",
            "         1.0       0.05      0.11      0.07        28\n",
            "\n",
            "    accuracy                           0.74       307\n",
            "   macro avg       0.48      0.46      0.46       307\n",
            "weighted avg       0.82      0.74      0.78       307\n",
            "\n",
            "[[225  54]\n",
            " [ 25   3]]\n",
            "Predictions saved to 'predicted_test_results.csv'\n"
          ]
        }
      ]
    }
  ]
}